{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fd27a-2c64-48ad-9b25-370c02d4bc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/marat/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/marat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import wikiextractor\n",
    "\n",
    "\n",
    "def _remove_non_printed_chars(string):\n",
    "    reg = re.compile('[^a-zA-Zа-яА-ЯёЁ]')\n",
    "    return reg.sub(' ', string)\n",
    "\n",
    "def _remove_stop_words(string,sw=[]):\n",
    "    return ' '.join([word if word not in sw else '' \\\n",
    "                     for word in string.strip().split(' ')])\n",
    "\n",
    "def _trim_string(string):\n",
    "    # remove extra spaces, remove trailing spaces, lower the case \n",
    "    return re.sub('\\s+',' ',string).strip().lower()\n",
    "    \n",
    "def clean_string(string,\n",
    "                 stop_words_list,\n",
    "                 min_len=2,\n",
    "                 max_len=30):\n",
    "\n",
    "    string = _remove_non_printed_chars(string)\n",
    "    string = _remove_stop_words(string,stop_words_list)\n",
    "    string = _trim_string(string)\n",
    "    # also remove short words, most likely containing addresses / crap / left-overs / etc remaining after removal\n",
    "    # gensim mostly does the same as above, it is used here for simplicity\n",
    "    string = ' '.join(gensim.utils.simple_preprocess(string,\n",
    "                                                     min_len=min_len,\n",
    "                                                     max_len=max_len))\n",
    "    return string\n",
    "    \n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "    \n",
    "def remove_special_chars(text,char_list):\n",
    "    for char in char_list:\n",
    "        text=text.replace(char,'')\n",
    "    return text.replace(u'\\xa0', u' ')\n",
    "\n",
    "def splitkeepsep(s, sep):\n",
    "    cleaned = []\n",
    "    s = re.split(\"(%s)\" % re.escape(sep), s)\n",
    "    for _ in s:\n",
    "        if _!='' and _!=sep:\n",
    "            cleaned.append(sep+_)\n",
    "    return cleaned\n",
    "\n",
    "def extract_url(text):\n",
    "    pattern = 'http([^\"]+)'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        url = match.group(0)\n",
    "        return url\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def create_vector(text):\n",
    "    return model.encode(text, normalize_embeddings=True)\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL статьи Википедии\n",
    "url = 'https://ru.wikipedia.org/wiki?curid=9'\n",
    "\n",
    "def getHeadings(url):\n",
    "    # Получаем содержимое страницы\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Парсим HTML-код с помощью BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Находим элемент с оглавлением (обычно он находится внутри элемента с классом mw-parser-output)\n",
    "    toc = soup.find('div', id='toc')\n",
    "    if toc is None:\n",
    "        return None\n",
    "    \n",
    "    # Извлекаем все элементы списка (<li>) из оглавления\n",
    "    items = toc.find_all('li')\n",
    "    \n",
    "    # Формируем список заголовков\n",
    "    headings = []\n",
    "    for item in items:\n",
    "        link = item.find('a')  # находим ссылку внутри каждого пункта списка\n",
    "        if link is not None:\n",
    "            heading_text = link.text.strip()  # получаем текст ссылки\n",
    "            cleaned_heading = heading_text.split(maxsplit=1)[-1].strip()  # убираем номер и точку\n",
    "            if cleaned_heading + '.' not in headings:\n",
    "                headings.append(cleaned_heading + '.')  # добавляем очищенное название в список\n",
    "    \n",
    "    # Выводим результат\n",
    "    return headings\n",
    "\n",
    "\n",
    "def process_wiki_files(wiki_file):\n",
    "    chars = ['\\n\\n']\n",
    "    global sw\n",
    "\n",
    "    with open(wiki_file, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    articles = splitkeepsep(content,'<doc id=')\n",
    "    df_texts = pd.DataFrame(columns=['article_uuid','url', 'title', 'article','proc_article','proc_len'])\n",
    "    emds = []\n",
    "\n",
    "    for article in articles:\n",
    "        if len(article) < 500:\n",
    "            continue\n",
    "\n",
    "        uuid_text = uuid4()\n",
    "        \n",
    "        articleParts = article.split('\\n')\n",
    "        url = extract_url(article)\n",
    "        headings = getHeadings(url)\n",
    "        if headings is None:\n",
    "            continue\n",
    "        title = articleParts[1]\n",
    "\n",
    "        article = remove_html_tags(article)\n",
    "        article = remove_special_chars(article, chars)\n",
    "        clearArticleParts = article.split('\\n')\n",
    "        \n",
    "        startIndex = 1\n",
    "        currHeading = ''\n",
    "        \n",
    "        for endIndex in range(startIndex, len(clearArticleParts)):\n",
    "            if len(clearArticleParts[endIndex]) < 100 and clearArticleParts[endIndex] in headings: \n",
    "                if endIndex - startIndex == 1:\n",
    "                    startIndex = endIndex\n",
    "                    currHeading = clearArticleParts[endIndex]\n",
    "                    continue\n",
    "            \n",
    "                onePart = title + '. ' + currHeading + ' ' + ' '.join(clearArticleParts[startIndex+1:endIndex])\n",
    "            \n",
    "                proc_onePart = clean_string(onePart, sw_ru)\n",
    "                proc_len = len(proc_onePart.split(' '))\n",
    "            \n",
    "                temp_df_texts = pd.DataFrame(\n",
    "                    {'article_uuid': [uuid_text],\n",
    "                     'url': url + \"#\" + currHeading[:-1].replace(' ', '_') if len(currHeading) > 0 else url,\n",
    "                     'title': title + '. ' + currHeading if len(currHeading) > 0 else title,\n",
    "                     'article': onePart,\n",
    "                     'proc_article':proc_onePart,\n",
    "                     'proc_len':proc_len\n",
    "                    })\n",
    "                df_texts = pd.concat([df_texts, temp_df_texts], ignore_index=True)\n",
    "            \n",
    "                emb = create_vector(proc_onePart)\n",
    "                emds.append(emb)\n",
    "            \n",
    "                startIndex = endIndex\n",
    "                currHeading = clearArticleParts[endIndex]\n",
    "    \n",
    "    return df_texts, np.array(emds)\n",
    "\n",
    "sw_en = set(stopwords.words('english'))\n",
    "sw_ru = set(stopwords.words('russian'))\n",
    "sw = list(sw_ru.union(sw_en))\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "from os.path import exists\n",
    "\n",
    "def saveEmbdsToVectorDB(embds, path):\n",
    "    if not exists(path):\n",
    "        index = faiss.IndexFlatL2(embds.shape[1]) \n",
    "        index = faiss.IndexIDMap(index)\n",
    "        index.add_with_ids(embds, np.arange(0, embds.shape[0]))\n",
    "        faiss.write_index(index, path)\n",
    "    else:\n",
    "        index = faiss.read_index(path)\n",
    "        index.add_with_ids(embds, np.arange(index.ntotal, index.ntotal + embds.shape[0]))\n",
    "        faiss.write_index(index, path)\n",
    "\n",
    "\n",
    "def getVectorDB(path):\n",
    "    return faiss.read_index(path)\n",
    "\n",
    "def addMetadataToDB(pathDB, cursor, conn, metadataDf):\n",
    "    metadataDf.to_sql(name='documents', con=conn, if_exists='append', index=False)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def getRevertedIndexTextDB(pathDB):\n",
    "    return open_dir(pathDB)\n",
    "\n",
    "\n",
    "def get_rows_from_csv(filename, indices):\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        header=None,\n",
    "        skiprows=lambda x: x not in indices\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def textSearch_with_bm25_ranking(query, pathDB):\n",
    "    index = getRevertedIndexTextDB(pathDB)\n",
    "    with index.searcher() as searcher:\n",
    "        query_parser = QueryParser(\"content\", index.schema)\n",
    "        parsed_query = query_parser.parse(query)\n",
    "        print(\"Получился запрос вида: \", parsed_query)\n",
    "        results = searcher.search(parsed_query)\n",
    "        return np.array([(result['id'], result.score) for result in results])\n",
    "\n",
    "\n",
    "wikiFilesRootPath = \"data/wiki\"\n",
    "vectorDBPath = 'data/data_bases/vectorDB.index'\n",
    "metadataDBPath = \"data/data_bases/documentsMetadataDB.db\"\n",
    "textsCsvPath = \"data/data_bases/texts.csv\"\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(\"Обрабатываю: \", file_path)\n",
    "    \n",
    "    # сохраняем в векторную БД\n",
    "    df_texts, embds = process_wiki_files(file_path)\n",
    "    saveEmbdsToVectorDB(embds, vectorDBPath)\n",
    "    currentDbSize = getVectorDB(vectorDBPath).ntotal\n",
    "    \n",
    "    # сохраняем тексты документов в текстовую БД\n",
    "    df_texts.to_csv(textsCsvPath, mode='a', header=False)\n",
    "\n",
    "    # сохраняем метаданные документов в SQLlite БД\n",
    "    conn = sqlite3.connect(metadataDBPath)\n",
    "    cursor = conn.cursor()\n",
    "    new_index = range(currentDbSize, currentDbSize + len(df_texts))\n",
    "    df_texts.index = new_index\n",
    "    addMetadataToDB(metadataDBPath, cursor, conn, df_texts[['url', 'title', 'proc_article']])\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# files_to_process = []\n",
    "# for dirpath, dirnames, filenames in os.walk(wikiFilesRootPath):\n",
    "#     for filename in filenames:\n",
    "#         file_path = os.path.join(dirpath, filename)\n",
    "#         files_to_process.append(file_path)\n",
    "\n",
    "#  # Используем ThreadPoolExecutor для параллельной обработки файлов\n",
    "# with ThreadPoolExecutor(max_workers=8) as executor:  # Количество рабочих потоков можно настроить\n",
    "#     futures = {executor.submit(process_file, file_path): file_path for file_path in files_to_process}\n",
    "    \n",
    "#     # Ждем завершения всех задач\n",
    "#     for future in concurrent.futures.as_completed(futures):\n",
    "#         file_path = futures[future]\n",
    "#         try:\n",
    "#             data = future.result()\n",
    "#         except Exception as exc:\n",
    "#             print(f'Ошибка при обработке файла {file_path}: {exc}')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Загрузка необходимых ресурсов\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "class DocsRanker(ABC):\n",
    "    @abstractmethod\n",
    "    def rankDocuments(self, query, docs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Bm25Ranker(DocsRanker):\n",
    "    # preprocess_func: переобразует запрос и документ в список слов\n",
    "    def __init__(self, preprocess_func = None) -> None:\n",
    "        self.preprocess_func = preprocess_func\n",
    "\n",
    "    def rankDocuments(self, query, docs):\n",
    "        if self.preprocess_func is None:\n",
    "            self.preprocess_func = lambda doc: doc.split()\n",
    "        tokenized_corpus = [self.preprocess_func(doc) for doc in docs]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "        tokenized_query = self.preprocess_func(query)\n",
    "        return bm25.get_scores(tokenized_query)\n",
    "\n",
    "def lemmatize(doc):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return [morph.parse(word)[0].normal_form for word in doc.split()]\n",
    "\n",
    "def stem(doc):\n",
    "   stemmer = SnowballStemmer(\"russian\")\n",
    "   words = nltk.word_tokenize(doc, language=\"russian\")\n",
    "   return [stemmer.stem(word) for word in words]\n",
    "\n",
    "\n",
    "class CrossEncoderRanker(DocsRanker):\n",
    "    def __init__(self) -> None:\n",
    "        self.reranker_model = CrossEncoder('DiTy/cross-encoder-russian-msmarco', max_length=512, device='cuda')\n",
    "        # self.reranker_model = CrossEncoder('DiTy/cross-encoder-russian-msmarco', max_length=512, device='cpu')\n",
    "\n",
    "    def rankDocuments(self, query, docs):\n",
    "        return np.array([self.reranker_model.predict([[query, doc]])[0] for doc in docs])\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pymorphy2\n",
    "\n",
    "def findVectorsIndexes(query, encoder, kDocuments):\n",
    "  queryEmbd = encoder.encode(query, normalize_embeddings=True)\n",
    "  D, I = index.search(np.array([queryEmbd]), kDocuments)\n",
    "  return I[0]\n",
    "\n",
    "def retrieveDocsAndUrls(indexes):\n",
    "  urlsAndDocs = get_rows_from_csv(textsCsvPath, indexes)[[2, 5]]\n",
    "  urlsAndDocs = urlsAndDocs.fillna('stub')\n",
    "  return urlsAndDocs[2], urlsAndDocs[5]\n",
    "\n",
    "def rankDocuments(query, indexes, ranker):\n",
    "    urls, docs = retrieveDocsAndUrls(indexes)\n",
    "    doc_scores = ranker.rankDocuments(query, docs)\n",
    "    sorted_idx = np.argsort(doc_scores)\n",
    "    return list(docs.iloc[sorted_idx[::-1]]), list(urls.iloc[sorted_idx[::-1]]), doc_scores[sorted_idx[::-1]]\n",
    "\n",
    "def getSortedDocumentsWithUrls(query, encoder, kDocuments, ranker):\n",
    "  indexes = findVectorsIndexes(query, encoder, kDocuments)\n",
    "  return rankDocuments(query, indexes, ranker)\n",
    "\n",
    "def getUnsortedDocumentsWithUrls(query, encoder, kDocuments):\n",
    "  indexes = findVectorsIndexes(query, encoder, kDocuments)\n",
    "  return retrieveDocsAndUrls(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dedecbf-a31a-4a47-b6b4-b950d489dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu_index = True\n",
    "  \n",
    "res = faiss.StandardGpuResources()\n",
    "index = getVectorDB(vectorDBPath)\n",
    "if use_gpu_index:\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e042d0-6170-4866-bd5d-b492935da7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(filename, ranker = None, document_num = 50):\n",
    "    real_urls = []\n",
    "    queries = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        prev_line = ''\n",
    "        for line in f:\n",
    "            if (prev_line == '##\\n'):\n",
    "                real_urls.append(line[:-1])\n",
    "            if (prev_line == '#\\n'):\n",
    "                queries.append(line[:-1])\n",
    "            prev_line = line\n",
    "    pos_arr = -np.ones(len(queries))\n",
    "    time_arr = np.zeros(len(queries))\n",
    "    if ranker is None:\n",
    "        N = len(queries)\n",
    "        for i in range(N):\n",
    "            start_time = time.time()\n",
    "            indexes = findVectorsIndexes(queries[i], model, document_num)[::-1]\n",
    "            end_time = time.time()\n",
    "            time_arr[i] = end_time - start_time\n",
    "            urls, docs = retrieveDocsAndUrls(indexes)\n",
    "            start_time = time.time()\n",
    "            sorted_idx = np.argsort(indexes)\n",
    "            end_time = time.time()\n",
    "            anses = urls.iloc[sorted_idx[::-1]].to_numpy()\n",
    "            for j in range(anses.shape[0]):\n",
    "                if anses[j] == real_urls[i]:\n",
    "                    pos_arr[i] = j\n",
    "                    break\n",
    "    else:\n",
    "        N = len(queries)\n",
    "        for i in range(N):\n",
    "            start_time = time.time()\n",
    "            indexes = findVectorsIndexes(queries[i], model, document_num)\n",
    "            end_time = time.time()\n",
    "            time_arr[i] = end_time - start_time\n",
    "            urls, docs = retrieveDocsAndUrls(indexes)\n",
    "            start_time = time.time()\n",
    "            doc_scores = ranker.rankDocuments(queries[i], docs)\n",
    "            sorted_idx = np.argsort(doc_scores)\n",
    "            end_time = time.time()\n",
    "            anses = urls.iloc[sorted_idx[::-1]].to_numpy()\n",
    "            time_arr[i] += end_time - start_time\n",
    "            for j in range(anses.shape[0]):\n",
    "                if anses[j] == real_urls[i]:\n",
    "                    pos_arr[i] = j\n",
    "                    break\n",
    "    return pos_arr, time_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb81c50-8672-4dae-bb90-4d18c22b6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_inv(n, coef = 5):\n",
    "    if n == -1:\n",
    "        return 0\n",
    "    else:\n",
    "        return coef / (n + coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ebe1b18-a983-4390-8fdc-7b6fb5a57c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(filename, ranker = None, echo = False, document_num = 50, metric = metric_inv):\n",
    "    p, t = test_model(filename, ranker = ranker, document_num = document_num)\n",
    "    if echo:\n",
    "        print(p)\n",
    "        print(t)\n",
    "    for i in range(p.shape[0]):\n",
    "        p[i] = metric(p[i])\n",
    "    return {'score' : p.mean(), 'avg_t' : t.mean(), 'std_t' : t.std()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f3e785-4fdd-4505-9c97-5046d5c31155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_eval(filenames, rankers, document_nums, metrics):\n",
    "    for filename in filenames:\n",
    "        for ranker in rankers:\n",
    "            for document_num in document_nums:\n",
    "                for metric in metrics:\n",
    "                    print(f'filename: {filename[0]}, ranker: {ranker[0]}, doc num: {document_num}, metric: {metric[0]}, ')\n",
    "                    print(eval_model(filename[1], ranker = ranker[1], document_num = document_num, metric = metric[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdba2b1b-6925-4ea7-98f7-c291ff154072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: Литва, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.4976757369614513, 'avg_t': 0.1854288305555071, 'std_t': 0.030290228096442686}\n",
      "filename: Литва, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.42307731950589095, 'avg_t': 0.1822347777230399, 'std_t': 0.04680426847220043}\n",
      "filename: Литва, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.26655455635299413, 'avg_t': 0.1763831002371652, 'std_t': 0.007981322130825738}\n",
      "filename: Литва, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.48486394557823126, 'avg_t': 0.19175197056361606, 'std_t': 0.032426700603974054}\n",
      "filename: Литва, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.4964317428603142, 'avg_t': 0.1794096061161586, 'std_t': 0.00843924462752806}\n",
      "filename: Литва, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.4481245128715881, 'avg_t': 0.1855290004185268, 'std_t': 0.02140704646733572}\n",
      "filename: Литва, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.5988095238095238, 'avg_t': 1.114317832674299, 'std_t': 0.4062452333201513}\n",
      "filename: Литва, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.6954081632653062, 'avg_t': 2.2010826451437815, 'std_t': 0.7702168317498124}\n",
      "filename: Литва, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.7568089950077527, 'avg_t': 9.188086639131818, 'std_t': 2.7614709238062938}\n",
      "filename: Лесков, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.2329365079365079, 'avg_t': 0.16912053823471068, 'std_t': 0.003832766085942885}\n",
      "filename: Лесков, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.18344155844155846, 'avg_t': 0.17976471185684204, 'std_t': 0.009271604870242027}\n",
      "filename: Лесков, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.08294284899962054, 'avg_t': 0.18066079616546632, 'std_t': 0.00787553140361164}\n",
      "filename: Лесков, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.22212301587301586, 'avg_t': 0.17277910709381103, 'std_t': 0.007860506796764914}\n",
      "filename: Лесков, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.18063325563325566, 'avg_t': 0.17806893587112427, 'std_t': 0.007883235548330153}\n",
      "filename: Лесков, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.18122908280803016, 'avg_t': 0.1891271471977234, 'std_t': 0.008028399774679314}\n",
      "filename: Лесков, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.2773809523809524, 'avg_t': 1.2777926445007324, 'std_t': 0.3688749206208613}\n",
      "filename: Лесков, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.2611111111111112, 'avg_t': 2.322088623046875, 'std_t': 0.6721164782620676}\n",
      "filename: Лесков, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.3238721804511278, 'avg_t': 10.990591824054718, 'std_t': 2.129730867623676}\n",
      "filename: Метро, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.5214947089947091, 'avg_t': 0.17763605382707384, 'std_t': 0.007903454583358359}\n",
      "filename: Метро, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.5726102601102601, 'avg_t': 0.18264069822099474, 'std_t': 0.022116611920663703}\n",
      "filename: Метро, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.23666071177694561, 'avg_t': 0.17253842618730333, 'std_t': 0.012348061199692508}\n",
      "filename: Метро, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.5933641975308643, 'avg_t': 0.17049107286665174, 'std_t': 0.008894361282523133}\n",
      "filename: Метро, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.7255291005291005, 'avg_t': 0.1722280051973131, 'std_t': 0.006759223835492281}\n",
      "filename: Метро, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.5372312665311386, 'avg_t': 0.17517101764678955, 'std_t': 0.005214889763112583}\n",
      "filename: Метро, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.6507936507936508, 'avg_t': 0.8976128233803643, 'std_t': 0.2074103476771692}\n",
      "filename: Метро, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.835978835978836, 'avg_t': 1.6616902616288927, 'std_t': 0.3600115503747701}\n",
      "filename: Метро, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.7992724867724869, 'avg_t': 7.425728612475925, 'std_t': 1.2339623358953313}\n",
      "filename: Перестройка, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.3765262515262515, 'avg_t': 0.17247759378873384, 'std_t': 0.007467972516805182}\n",
      "filename: Перестройка, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.3241758241758242, 'avg_t': 0.17311699573810285, 'std_t': 0.007543420001004347}\n",
      "filename: Перестройка, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.12430906366290369, 'avg_t': 0.1827683265392597, 'std_t': 0.009973741184365628}\n",
      "filename: Перестройка, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.39789377289377287, 'avg_t': 0.18282314447256234, 'std_t': 0.010842005328687012}\n",
      "filename: Перестройка, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.3298368298368298, 'avg_t': 0.18375669992887056, 'std_t': 0.0056588125645373796}\n",
      "filename: Перестройка, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.2302114863475312, 'avg_t': 0.18332272309523362, 'std_t': 0.007370555761933566}\n",
      "filename: Перестройка, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.46153846153846156, 'avg_t': 1.313959158383883, 'std_t': 0.4359311814415823}\n",
      "filename: Перестройка, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.4487179487179488, 'avg_t': 2.2610232463249793, 'std_t': 0.762157613296569}\n",
      "filename: Перестройка, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.43525641025641026, 'avg_t': 10.572058659333448, 'std_t': 2.7217455709281344}\n"
     ]
    }
   ],
   "source": [
    "multi_eval(\n",
    "    [\n",
    "        ['Литва', 'data/queries_split/Литва.txt'],\n",
    "        ['Лесков', 'data/queries_split/Лесков.txt'],\n",
    "        ['Метро', 'data/queries_split/Метро.txt'],\n",
    "        ['Перестройка', 'data/queries_split/Перестройка.txt']\n",
    "    ],\n",
    "    [['None', None], ['Bm25', Bm25Ranker()], ['CrossEncoder', CrossEncoderRanker()]],\n",
    "    [5, 10, 50],\n",
    "    [['inv 5', metric_inv]]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f045f3e-6c45-4e65-ad45-7ff2c3ad4d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
