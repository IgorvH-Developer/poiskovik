{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07921f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLUMN = 5\n",
    "STEM_COLUMN = 7\n",
    "\n",
    "DOCS = DEFAULT_COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fd27a-2c64-48ad-9b25-370c02d4bc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/marat/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/marat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import faiss\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import wikiextractor\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Загрузка необходимых ресурсов\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "def _remove_non_printed_chars(string):\n",
    "    reg = re.compile('[^a-zA-Zа-яА-ЯёЁ]')\n",
    "    return reg.sub(' ', string)\n",
    "\n",
    "def _remove_stop_words(string,sw=[]):\n",
    "    return ' '.join([word if word not in sw else '' \\\n",
    "                     for word in string.strip().split(' ')])\n",
    "\n",
    "def _trim_string(string):\n",
    "    # remove extra spaces, remove trailing spaces, lower the case \n",
    "    return re.sub('\\s+',' ',string).strip().lower()\n",
    "    \n",
    "def clean_string(string,\n",
    "                 stop_words_list,\n",
    "                 min_len=2,\n",
    "                 max_len=30):\n",
    "\n",
    "    string = _remove_non_printed_chars(string)\n",
    "    string = _remove_stop_words(string,stop_words_list)\n",
    "    string = _trim_string(string)\n",
    "    # also remove short words, most likely containing addresses / crap / left-overs / etc remaining after removal\n",
    "    # gensim mostly does the same as above, it is used here for simplicity\n",
    "    string = ' '.join(gensim.utils.simple_preprocess(string,\n",
    "                                                     min_len=min_len,\n",
    "                                                     max_len=max_len))\n",
    "    return string\n",
    "    \n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "    \n",
    "def remove_special_chars(text,char_list):\n",
    "    for char in char_list:\n",
    "        text=text.replace(char,'')\n",
    "    return text.replace(u'\\xa0', u' ')\n",
    "\n",
    "def splitkeepsep(s, sep):\n",
    "    cleaned = []\n",
    "    s = re.split(\"(%s)\" % re.escape(sep), s)\n",
    "    for _ in s:\n",
    "        if _!='' and _!=sep:\n",
    "            cleaned.append(sep+_)\n",
    "    return cleaned\n",
    "\n",
    "def extract_url(text):\n",
    "    pattern = 'http([^\"]+)'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        url = match.group(0)\n",
    "        return url\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def create_vector(text):\n",
    "    return model.encode(text, normalize_embeddings=True)\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL статьи Википедии\n",
    "url = 'https://ru.wikipedia.org/wiki?curid=9'\n",
    "\n",
    "def getHeadings(url):\n",
    "    # Получаем содержимое страницы\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Парсим HTML-код с помощью BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Находим элемент с оглавлением (обычно он находится внутри элемента с классом mw-parser-output)\n",
    "    toc = soup.find('div', id='toc')\n",
    "    if toc is None:\n",
    "        return None\n",
    "    \n",
    "    # Извлекаем все элементы списка (<li>) из оглавления\n",
    "    items = toc.find_all('li')\n",
    "    \n",
    "    # Формируем список заголовков\n",
    "    headings = []\n",
    "    for item in items:\n",
    "        link = item.find('a')  # находим ссылку внутри каждого пункта списка\n",
    "        if link is not None:\n",
    "            heading_text = link.text.strip()  # получаем текст ссылки\n",
    "            cleaned_heading = heading_text.split(maxsplit=1)[-1].strip()  # убираем номер и точку\n",
    "            if cleaned_heading + '.' not in headings:\n",
    "                headings.append(cleaned_heading + '.')  # добавляем очищенное название в список\n",
    "    \n",
    "    # Выводим результат\n",
    "    return headings\n",
    "\n",
    "\n",
    "def lemmatize(doc):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return [morph.parse(word)[0].normal_form for word in doc.split()]\n",
    "\n",
    "def stem(doc):\n",
    "   stemmer = SnowballStemmer(\"russian\")\n",
    "   return [stemmer.stem(word) for word in doc.split()]\n",
    "\n",
    "def process_wiki_files(wiki_file):\n",
    "    chars = ['\\n\\n']\n",
    "    global sw\n",
    "\n",
    "    with open(wiki_file, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    articles = splitkeepsep(content,'<doc id=')\n",
    "    df_texts = pd.DataFrame(columns=['article_uuid','url', 'title', 'article','proc_article','proc_len'])\n",
    "    emds = []\n",
    "\n",
    "    for article in articles:\n",
    "        if len(article) < 500:\n",
    "            continue\n",
    "\n",
    "        uuid_text = uuid4()\n",
    "        \n",
    "        articleParts = article.split('\\n')\n",
    "        url = extract_url(article)\n",
    "        headings = getHeadings(url)\n",
    "        if headings is None:\n",
    "            continue\n",
    "        title = articleParts[1]\n",
    "\n",
    "        article = remove_html_tags(article)\n",
    "        article = remove_special_chars(article, chars)\n",
    "        clearArticleParts = article.split('\\n')\n",
    "        \n",
    "        startIndex = 1\n",
    "        currHeading = ''\n",
    "        \n",
    "        for endIndex in range(startIndex, len(clearArticleParts)):\n",
    "            if len(clearArticleParts[endIndex]) < 100 and clearArticleParts[endIndex] in headings: \n",
    "                if endIndex - startIndex == 1:\n",
    "                    startIndex = endIndex\n",
    "                    currHeading = clearArticleParts[endIndex]\n",
    "                    continue\n",
    "            \n",
    "                onePart = title + '. ' + currHeading + ' ' + ' '.join(clearArticleParts[startIndex+1:endIndex])\n",
    "            \n",
    "                proc_onePart = clean_string(onePart, sw_ru)\n",
    "                stemmed_onePart = ' '.join(stem(proc_onePart))\n",
    "                proc_len = len(proc_onePart.split(' '))\n",
    "            \n",
    "                temp_df_texts = pd.DataFrame(\n",
    "                    {'article_uuid': [uuid_text],\n",
    "                     'url': url + \"#\" + currHeading[:-1].replace(' ', '_') if len(currHeading) > 0 else url,\n",
    "                     'title': title + '. ' + currHeading if len(currHeading) > 0 else title,\n",
    "                     'article': onePart,\n",
    "                     'proc_article':proc_onePart,\n",
    "                     'proc_len':proc_len,\n",
    "                     'stem_article':stemmed_onePart\n",
    "                    })\n",
    "                df_texts = pd.concat([df_texts, temp_df_texts], ignore_index=True)\n",
    "            \n",
    "                emb = create_vector(proc_onePart)\n",
    "                emds.append(emb)\n",
    "            \n",
    "                startIndex = endIndex\n",
    "                currHeading = clearArticleParts[endIndex]\n",
    "    \n",
    "    return df_texts, np.array(emds)\n",
    "\n",
    "sw_en = set(stopwords.words('english'))\n",
    "sw_ru = set(stopwords.words('russian'))\n",
    "sw = list(sw_ru.union(sw_en))\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "from os.path import exists\n",
    "\n",
    "def saveEmbdsToVectorDB(embds, path):\n",
    "    if not exists(path):\n",
    "        index = faiss.IndexFlatL2(embds.shape[1]) \n",
    "        index = faiss.IndexIDMap(index)\n",
    "        index.add_with_ids(embds, np.arange(0, embds.shape[0]))\n",
    "        faiss.write_index(index, path)\n",
    "    else:\n",
    "        index = faiss.read_index(path)\n",
    "        index.add_with_ids(embds, np.arange(index.ntotal, index.ntotal + embds.shape[0]))\n",
    "        faiss.write_index(index, path)\n",
    "\n",
    "\n",
    "def getVectorDB(path):\n",
    "    return faiss.read_index(path)\n",
    "\n",
    "def addMetadataToDB(pathDB, cursor, conn, metadataDf):\n",
    "    metadataDf.to_sql(name='documents', con=conn, if_exists='append', index=False)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def getRevertedIndexTextDB(pathDB):\n",
    "    return open_dir(pathDB)\n",
    "\n",
    "\n",
    "def get_rows_from_csv(filename, indices):\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        header=None,\n",
    "        skiprows=lambda x: x not in indices\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def textSearch_with_bm25_ranking(query, pathDB):\n",
    "    index = getRevertedIndexTextDB(pathDB)\n",
    "    with index.searcher() as searcher:\n",
    "        query_parser = QueryParser(\"content\", index.schema)\n",
    "        parsed_query = query_parser.parse(query)\n",
    "        print(\"Получился запрос вида: \", parsed_query)\n",
    "        results = searcher.search(parsed_query)\n",
    "        return np.array([(result['id'], result.score) for result in results])\n",
    "\n",
    "\n",
    "wikiFilesRootPath = \"data/wiki\"\n",
    "vectorDBPath = 'data/data_bases/vectorDB.index'\n",
    "metadataDBPath = \"data/data_bases/documentsMetadataDB.db\"\n",
    "textsCsvPath = \"data/data_bases/texts.csv\"\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(\"Обрабатываю: \", file_path)\n",
    "    \n",
    "    # сохраняем в векторную БД\n",
    "    df_texts, embds = process_wiki_files(file_path)\n",
    "    saveEmbdsToVectorDB(embds, vectorDBPath)\n",
    "    currentDbSize = getVectorDB(vectorDBPath).ntotal\n",
    "    \n",
    "    # сохраняем тексты документов в текстовую БД\n",
    "    df_texts.to_csv(textsCsvPath, mode='a', header=False)\n",
    "\n",
    "    # сохраняем метаданные документов в SQLlite БД\n",
    "    conn = sqlite3.connect(metadataDBPath)\n",
    "    cursor = conn.cursor()\n",
    "    new_index = range(currentDbSize, currentDbSize + len(df_texts))\n",
    "    df_texts.index = new_index\n",
    "    addMetadataToDB(metadataDBPath, cursor, conn, df_texts[['url', 'title', 'proc_article']])\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# files_to_process = []\n",
    "# for dirpath, dirnames, filenames in os.walk(wikiFilesRootPath):\n",
    "#     for filename in filenames:\n",
    "#         file_path = os.path.join(dirpath, filename)\n",
    "#         files_to_process.append(file_path)\n",
    "\n",
    "#  # Используем ThreadPoolExecutor для параллельной обработки файлов\n",
    "# with ThreadPoolExecutor(max_workers=8) as executor:  # Количество рабочих потоков можно настроить\n",
    "#     futures = {executor.submit(process_file, file_path): file_path for file_path in files_to_process}\n",
    "    \n",
    "#     # Ждем завершения всех задач\n",
    "#     for future in concurrent.futures.as_completed(futures):\n",
    "#         file_path = futures[future]\n",
    "#         try:\n",
    "#             data = future.result()\n",
    "#         except Exception as exc:\n",
    "#             print(f'Ошибка при обработке файла {file_path}: {exc}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DocsRanker(ABC):\n",
    "    @abstractmethod\n",
    "    def rankDocuments(self, query, docs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Bm25Ranker(DocsRanker):\n",
    "    # preprocess_func: переобразует запрос и документ в список слов\n",
    "    def __init__(self, bm25_alg = BM25Okapi,  preprocess_func = None) -> None:\n",
    "        self.preprocess_func = preprocess_func\n",
    "        self.bm25_alg = bm25_alg\n",
    "\n",
    "    def rankDocuments(self, query, docs):\n",
    "        if self.preprocess_func is None:\n",
    "            self.preprocess_func = lambda doc: doc.split()\n",
    "        tokenized_corpus = [doc.split() for doc in docs]\n",
    "        bm25 = self.bm25_alg(tokenized_corpus)\n",
    "        tokenized_query = self.preprocess_func(query)\n",
    "        return bm25.get_scores(tokenized_query)\n",
    "    \n",
    "class BiEncoderRanker(DocsRanker):\n",
    "    def __init__(self) -> None:\n",
    "        self.reranker_model = SentenceTransformer('DiTy/bi-encoder-russian-msmarco', device='cuda')\n",
    "\n",
    "\n",
    "    def rankDocuments(self, query, docs):\n",
    "        sentences = [query] + docs\n",
    "        embeddings = model.encode(sentences)\n",
    "        results = util.semantic_search(embeddings[0], embeddings[1:])[0]\n",
    "        return np.array([res['score'] for res in results])\n",
    "\n",
    "\n",
    "class CrossEncoderRanker(DocsRanker):\n",
    "    def __init__(self) -> None:\n",
    "        self.reranker_model = CrossEncoder('DiTy/cross-encoder-russian-msmarco', max_length=512, device='cuda')\n",
    "        # self.reranker_model = CrossEncoder('DiTy/cross-encoder-russian-msmarco', max_length=512, device='cpu')\n",
    "\n",
    "    def rankDocuments(self, query, docs):\n",
    "        return np.array([self.reranker_model.predict([[query, doc]])[0] for doc in docs])\n",
    "\n",
    "\n",
    "def findVectorsIndexes(query, encoder, kDocuments):\n",
    "  queryEmbd = encoder.encode(query, normalize_embeddings=True)\n",
    "  D, I = index.search(np.array([queryEmbd]), kDocuments)\n",
    "  return D[0], I[0]\n",
    "\n",
    "def retrieveDocsAndUrls(indexes):\n",
    "  urlsAndDocs = get_rows_from_csv(textsCsvPath, indexes)[[2, DOCS]]\n",
    "  urlsAndDocs = urlsAndDocs.fillna('stub')\n",
    "  return urlsAndDocs[2], urlsAndDocs[DOCS]\n",
    "\n",
    "def rankDocuments(query, indexes, ranker):\n",
    "    urls, docs = retrieveDocsAndUrls(indexes)\n",
    "    doc_scores = ranker.rankDocuments(query, docs)\n",
    "    sorted_idx = np.argsort(doc_scores)\n",
    "    return list(docs.iloc[sorted_idx[::-1]]), list(urls.iloc[sorted_idx[::-1]]), doc_scores[sorted_idx[::-1]]\n",
    "\n",
    "def getSortedDocumentsWithUrls(query, encoder, kDocuments, ranker):\n",
    "  indexes = findVectorsIndexes(query, encoder, kDocuments)\n",
    "  return rankDocuments(query, indexes, ranker)\n",
    "\n",
    "def getUnsortedDocumentsWithUrls(query, encoder, kDocuments):\n",
    "  indexes = findVectorsIndexes(query, encoder, kDocuments)\n",
    "  return retrieveDocsAndUrls(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class BM25WithProximity:\n",
    "    def __init__(self, documents: List[List[str]], k1=1.5, b=0.75, proximity_weight=0.6):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.proximity_weight = proximity_weight\n",
    "        self.documents = documents\n",
    "        self.doc_lengths = [len(doc) for doc in self.documents]\n",
    "        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths)\n",
    "        self.total_docs = len(self.documents)\n",
    "\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        for doc in self.documents:\n",
    "            for term in set(doc):\n",
    "                self.doc_freqs[term] += 1\n",
    "\n",
    "    def _bm25_term_score(self, term, doc, doc_length):\n",
    "        term_freq = doc.count(term)\n",
    "        if term_freq == 0:\n",
    "            return 0\n",
    "        idf = math.log((self.total_docs) /\n",
    "                       (self.doc_freqs[term] + 1.0) + 1)\n",
    "        normalization = 1 - self.b + self.b * (doc_length / self.avg_doc_length)\n",
    "        score = idf * ((term_freq * (self.k1 + 1)) /\n",
    "                       (term_freq + self.k1 * normalization))\n",
    "        return score\n",
    "\n",
    "    def _proximity_score(self, query_terms, doc):\n",
    "        positions = {term: [] for term in query_terms}\n",
    "        for index, word in enumerate(doc):\n",
    "            if word in query_terms:\n",
    "                positions[word].append(index)\n",
    "\n",
    "\n",
    "        min_distance = float('inf')\n",
    "        for i, term1 in enumerate(query_terms):\n",
    "            for term2 in query_terms[i + 1:]:\n",
    "                for pos1 in positions[term1]:\n",
    "                    for pos2 in positions[term2]:\n",
    "                        distance = abs(pos1 - pos2)\n",
    "                        if distance < min_distance:\n",
    "                            min_distance = distance\n",
    "\n",
    "\n",
    "        if min_distance == float('inf'):\n",
    "            return 0\n",
    "        return 1 / (1 + min_distance)\n",
    "\n",
    "    def score(self, query: List[str], doc_index: int):\n",
    "        doc = self.documents[doc_index]\n",
    "        doc_length = self.doc_lengths[doc_index]\n",
    "\n",
    "        bm25_score = sum(self._bm25_term_score(term, doc, doc_length) for term in query)\n",
    "\n",
    "        proximity_score = self._proximity_score(query, doc)\n",
    "\n",
    "        total_score = bm25_score + self.proximity_weight * proximity_score\n",
    "        return total_score\n",
    "\n",
    "    def get_scores(self, query: List[str]):\n",
    "        scores = [self.score(query, doc_index) for doc_index in range(self.total_docs)]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4778a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def calculate_relevance(query_words: List[str], document_words: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Оценивает релевантность документа запросу на основе кворума (числа совпавших слов).\n",
    "    \n",
    "    :param query: текст запроса\n",
    "    :param document: текст документа\n",
    "    :return: значение релевантности от 0 до 1\n",
    "    \"\"\"\n",
    "    if not query_words:\n",
    "        return 0.0\n",
    "\n",
    "    query_words = set(query_words)\n",
    "    document_words = set(document_words)\n",
    "    \n",
    "    intersection = query_words.intersection(document_words)\n",
    "    relevance = len(intersection) / len(query_words)\n",
    "    return relevance\n",
    "\n",
    "def documents_filter_quorum(query: List[str], documents: List[List[str]], threshold: float = 0.5) -> List[str]:\n",
    "    return [doc for doc in documents if calculate_relevance(query, doc) >= threshold]\n",
    "\n",
    "\n",
    "query = \"модель машинного обучения\".lower().split()\n",
    "documents = [\n",
    "    \"Машинное обучение и искусственный интеллект\".lower().split(),\n",
    "    \"Основы программирования\".lower().split(),\n",
    "    \"Модель обучения нейронных сетей\".lower().split(),\n",
    "    \"Машинное обучение: теория и практика\".lower().split(),\n",
    "]\n",
    "\n",
    "relevant_docs = documents_filter_quorum(query, documents, threshold=0.25)\n",
    "print(f'Релевантные документы: {relevant_docs}')\n",
    "# Релевантные документы: [['модель', 'обучения', 'нейронных', 'сетей']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dedecbf-a31a-4a47-b6b4-b950d489dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu_index = True\n",
    "  \n",
    "res = faiss.StandardGpuResources()\n",
    "index = getVectorDB(vectorDBPath)\n",
    "if use_gpu_index:\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e042d0-6170-4866-bd5d-b492935da7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(filename, ranker = None, document_num = 50):\n",
    "    real_urls = []\n",
    "    queries = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        prev_line = ''\n",
    "        for line in f:\n",
    "            if (prev_line == '##\\n'):\n",
    "                real_urls.append(line[:-1])\n",
    "            if (prev_line == '#\\n'):\n",
    "                queries.append(line[:-1])\n",
    "            prev_line = line\n",
    "    pos_arr = -np.ones(len(queries))\n",
    "    time_arr = np.zeros(len(queries))\n",
    "    if ranker is None:\n",
    "        N = len(queries)\n",
    "        for i in range(N):\n",
    "            start_time = time.time()\n",
    "            dists, indexes = findVectorsIndexes(queries[i], model, document_num)\n",
    "            end_time = time.time()\n",
    "            print(dists)\n",
    "            print(indexes)\n",
    "            time_arr[i] = end_time - start_time\n",
    "            alter_res = []\n",
    "            for idx in indexes:\n",
    "                alter_url, _ = retrieveDocsAndUrls([idx])\n",
    "                alter_res.append(alter_url)\n",
    "            print(alter_res)\n",
    "            urls, docs = retrieveDocsAndUrls(indexes)\n",
    "            start_time = time.time()\n",
    "            sorted_idx = np.argsort(indexes)\n",
    "            end_time = time.time()\n",
    "            print(urls)\n",
    "            anses = urls.iloc[sorted_idx].to_numpy()\n",
    "            #print(anses)\n",
    "            for j in range(anses.shape[0]):\n",
    "                if anses[j] == real_urls[i]:\n",
    "                    pos_arr[i] = j\n",
    "                    break\n",
    "    else:\n",
    "        N = len(queries)\n",
    "        for i in range(N):\n",
    "            start_time = time.time()\n",
    "            _, indexes = findVectorsIndexes(queries[i], model, document_num)\n",
    "            end_time = time.time()\n",
    "            time_arr[i] = end_time - start_time\n",
    "            urls, docs = retrieveDocsAndUrls(indexes)\n",
    "            start_time = time.time()\n",
    "            doc_scores = ranker.rankDocuments(queries[i], docs)\n",
    "            sorted_idx = np.argsort(doc_scores)\n",
    "            end_time = time.time()\n",
    "            anses = urls.iloc[sorted_idx[::-1]].to_numpy()\n",
    "            time_arr[i] += end_time - start_time\n",
    "            for j in range(anses.shape[0]):\n",
    "                if anses[j] == real_urls[i]:\n",
    "                    pos_arr[i] = j\n",
    "                    break\n",
    "    return pos_arr, time_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb81c50-8672-4dae-bb90-4d18c22b6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_inv(n, coef = 5):\n",
    "    if n == -1:\n",
    "        return 0\n",
    "    else:\n",
    "        return coef / (n + coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ebe1b18-a983-4390-8fdc-7b6fb5a57c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(filename, ranker = None, echo = False, document_num = 50, metric = metric_inv):\n",
    "    p, t = test_model(filename, ranker = ranker, document_num = document_num)\n",
    "    if echo:\n",
    "        print(p)\n",
    "        print(t)\n",
    "    for i in range(p.shape[0]):\n",
    "        p[i] = metric(p[i])\n",
    "    return {'score' : p.mean(), 'avg_t' : t.mean(), 'std_t' : t.std()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f3e785-4fdd-4505-9c97-5046d5c31155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_eval(filenames, rankers, document_nums, metrics):\n",
    "    for filename in filenames:\n",
    "        for ranker in rankers:\n",
    "            for document_num in document_nums:\n",
    "                for metric in metrics:\n",
    "                    print(f'filename: {filename[0]}, ranker: {ranker[0]}, doc num: {document_num}, metric: {metric[0]}, ')\n",
    "                    print(eval_model(filename[1], ranker = ranker[1], document_num = document_num, metric = metric[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba2b1b-6925-4ea7-98f7-c291ff154072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: Литва, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.4976757369614513, 'avg_t': 0.1854288305555071, 'std_t': 0.030290228096442686}\n",
      "filename: Литва, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.42307731950589095, 'avg_t': 0.1822347777230399, 'std_t': 0.04680426847220043}\n",
      "filename: Литва, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.26655455635299413, 'avg_t': 0.1763831002371652, 'std_t': 0.007981322130825738}\n",
      "filename: Литва, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.48486394557823126, 'avg_t': 0.19175197056361606, 'std_t': 0.032426700603974054}\n",
      "filename: Литва, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.4964317428603142, 'avg_t': 0.1794096061161586, 'std_t': 0.00843924462752806}\n",
      "filename: Литва, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.4481245128715881, 'avg_t': 0.1855290004185268, 'std_t': 0.02140704646733572}\n",
      "filename: Литва, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.5988095238095238, 'avg_t': 1.114317832674299, 'std_t': 0.4062452333201513}\n",
      "filename: Литва, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.6954081632653062, 'avg_t': 2.2010826451437815, 'std_t': 0.7702168317498124}\n",
      "filename: Литва, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.7568089950077527, 'avg_t': 9.188086639131818, 'std_t': 2.7614709238062938}\n",
      "filename: Лесков, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.2329365079365079, 'avg_t': 0.16912053823471068, 'std_t': 0.003832766085942885}\n",
      "filename: Лесков, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.18344155844155846, 'avg_t': 0.17976471185684204, 'std_t': 0.009271604870242027}\n",
      "filename: Лесков, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.08294284899962054, 'avg_t': 0.18066079616546632, 'std_t': 0.00787553140361164}\n",
      "filename: Лесков, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.22212301587301586, 'avg_t': 0.17277910709381103, 'std_t': 0.007860506796764914}\n",
      "filename: Лесков, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.18063325563325566, 'avg_t': 0.17806893587112427, 'std_t': 0.007883235548330153}\n",
      "filename: Лесков, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.18122908280803016, 'avg_t': 0.1891271471977234, 'std_t': 0.008028399774679314}\n",
      "filename: Лесков, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.2773809523809524, 'avg_t': 1.2777926445007324, 'std_t': 0.3688749206208613}\n",
      "filename: Лесков, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.2611111111111112, 'avg_t': 2.322088623046875, 'std_t': 0.6721164782620676}\n",
      "filename: Лесков, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.3238721804511278, 'avg_t': 10.990591824054718, 'std_t': 2.129730867623676}\n",
      "filename: Метро, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.5214947089947091, 'avg_t': 0.17763605382707384, 'std_t': 0.007903454583358359}\n",
      "filename: Метро, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.5726102601102601, 'avg_t': 0.18264069822099474, 'std_t': 0.022116611920663703}\n",
      "filename: Метро, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.23666071177694561, 'avg_t': 0.17253842618730333, 'std_t': 0.012348061199692508}\n",
      "filename: Метро, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.5933641975308643, 'avg_t': 0.17049107286665174, 'std_t': 0.008894361282523133}\n",
      "filename: Метро, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.7255291005291005, 'avg_t': 0.1722280051973131, 'std_t': 0.006759223835492281}\n",
      "filename: Метро, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.5372312665311386, 'avg_t': 0.17517101764678955, 'std_t': 0.005214889763112583}\n",
      "filename: Метро, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.6507936507936508, 'avg_t': 0.8976128233803643, 'std_t': 0.2074103476771692}\n",
      "filename: Метро, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.835978835978836, 'avg_t': 1.6616902616288927, 'std_t': 0.3600115503747701}\n",
      "filename: Метро, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.7992724867724869, 'avg_t': 7.425728612475925, 'std_t': 1.2339623358953313}\n",
      "filename: Перестройка, ranker: None, doc num: 5, metric: inv 5, \n",
      "{'score': 0.3765262515262515, 'avg_t': 0.17247759378873384, 'std_t': 0.007467972516805182}\n",
      "filename: Перестройка, ranker: None, doc num: 10, metric: inv 5, \n",
      "{'score': 0.3241758241758242, 'avg_t': 0.17311699573810285, 'std_t': 0.007543420001004347}\n",
      "filename: Перестройка, ranker: None, doc num: 50, metric: inv 5, \n",
      "{'score': 0.12430906366290369, 'avg_t': 0.1827683265392597, 'std_t': 0.009973741184365628}\n",
      "filename: Перестройка, ranker: Bm25, doc num: 5, metric: inv 5, \n",
      "{'score': 0.39789377289377287, 'avg_t': 0.18282314447256234, 'std_t': 0.010842005328687012}\n",
      "filename: Перестройка, ranker: Bm25, doc num: 10, metric: inv 5, \n",
      "{'score': 0.3298368298368298, 'avg_t': 0.18375669992887056, 'std_t': 0.0056588125645373796}\n",
      "filename: Перестройка, ranker: Bm25, doc num: 50, metric: inv 5, \n",
      "{'score': 0.2302114863475312, 'avg_t': 0.18332272309523362, 'std_t': 0.007370555761933566}\n",
      "filename: Перестройка, ranker: CrossEncoder, doc num: 5, metric: inv 5, \n",
      "{'score': 0.46153846153846156, 'avg_t': 1.313959158383883, 'std_t': 0.4359311814415823}\n",
      "filename: Перестройка, ranker: CrossEncoder, doc num: 10, metric: inv 5, \n",
      "{'score': 0.4487179487179488, 'avg_t': 2.2610232463249793, 'std_t': 0.762157613296569}\n",
      "filename: Перестройка, ranker: CrossEncoder, doc num: 50, metric: inv 5, \n",
      "{'score': 0.43525641025641026, 'avg_t': 10.572058659333448, 'std_t': 2.7217455709281344}\n"
     ]
    }
   ],
   "source": [
    "multi_eval(\n",
    "    [\n",
    "        ['Литва', 'data/queries_split/Литва.txt'],\n",
    "        ['Лесков', 'data/queries_split/Лесков.txt'],\n",
    "        ['Метро', 'data/queries_split/Метро.txt'],\n",
    "        ['Перестройка', 'data/queries_split/Перестройка.txt']\n",
    "    ],\n",
    "    [['None', None], ['Bm25', Bm25Ranker(bm25_alg = BM25WithProximity, preprocess_func = stem)], ['CrossEncoder', CrossEncoderRanker()]],\n",
    "    [5, 10, 50],\n",
    "    [['inv 5', metric_inv]]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f045f3e-6c45-4e65-ad45-7ff2c3ad4d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46514064 0.48637408 0.5020088  0.5178219  0.5191462 ]\n",
      "[  569 27960 25554 36330 51794]\n",
      "[0    https://ru.wikipedia.org/wiki?curid=726#Геогра...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=79\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=38#Географ...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=22070#Геог...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=21837#Геог...\n",
      "Name: 2, dtype: object]\n",
      "0    https://ru.wikipedia.org/wiki?curid=726#Геогра...\n",
      "1    https://ru.wikipedia.org/wiki?curid=38#Географ...\n",
      "2               https://ru.wikipedia.org/wiki?curid=79\n",
      "3    https://ru.wikipedia.org/wiki?curid=22070#Геог...\n",
      "4    https://ru.wikipedia.org/wiki?curid=21837#Геог...\n",
      "Name: 2, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.]), array([0.5779624]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model('data/queries_split/short.txt', ranker = None, document_num = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3718712-7342-4b98-bffe-a6bbbd96c194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46514064 0.48637408 0.5020088  0.5178219  0.5191462  0.52117324\n",
      " 0.52175593 0.5274729  0.5305184  0.5454009 ]\n",
      "[  569 27960 25554 36330 51794 21051 44657 23783 18404 21242]\n",
      "[0    https://ru.wikipedia.org/wiki?curid=726#Геогра...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=79\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=38#Географ...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=22070#Геог...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=21837#Геог...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=548#География\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=18820#Геог...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=13269#Круп...\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=7\n",
      "Name: 2, dtype: object, 0    https://ru.wikipedia.org/wiki?curid=7317#Геогр...\n",
      "Name: 2, dtype: object]\n",
      "0    https://ru.wikipedia.org/wiki?curid=726#Геогра...\n",
      "1                https://ru.wikipedia.org/wiki?curid=7\n",
      "2    https://ru.wikipedia.org/wiki?curid=548#География\n",
      "3    https://ru.wikipedia.org/wiki?curid=7317#Геогр...\n",
      "4    https://ru.wikipedia.org/wiki?curid=13269#Круп...\n",
      "5    https://ru.wikipedia.org/wiki?curid=38#Географ...\n",
      "6               https://ru.wikipedia.org/wiki?curid=79\n",
      "7    https://ru.wikipedia.org/wiki?curid=22070#Геог...\n",
      "8    https://ru.wikipedia.org/wiki?curid=18820#Геог...\n",
      "9    https://ru.wikipedia.org/wiki?curid=21837#Геог...\n",
      "Name: 2, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([6.]), array([0.20453882]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model('data/queries_split/short.txt', ranker = None, document_num = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08a193-9e1c-4e15-affd-d72a5e79a163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
